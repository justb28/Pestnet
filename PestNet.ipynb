{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "610b9d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /home/ubuntu/unet_env\n",
      "Files in current directory: ['Data', 'Pnet.py', '.venv', 'predicted_masks', 'lib64', 'pyvenv.cfg', 'pyproject.toml', 'bin', 'etc', 'lib', 'pestnet.py', 'plant_segmentation_model.pth', 'share', 'uv.lock', 'out1.png', 'PestNet.ipynb', 'Untitled-1.ipynb', 'include']\n",
      "Data path exists: True\n",
      "Contents of data path: ['Natural', 'Mask']\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import zipfile\n",
    "from math import atan2, cos, sin, sqrt, pi, log\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image,ImageFile\n",
    "from numpy import linalg as LA\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "#Print current directory\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "print(f\"Files in current directory: {os.listdir('.')}\")\n",
    "\n",
    "# Try to access your data paths\n",
    "try:\n",
    "    # Modify this path based on the output of the current directory\n",
    "    data_path = os.path.join(os.getcwd(),\"Data\", \"Data1\")\n",
    "    print(f\"Data path exists: {os.path.exists(data_path)}\")\n",
    "    if os.path.exists(data_path):\n",
    "        print(f\"Contents of data path: {os.listdir(data_path)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fa1278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_folders(base_path, target_folder):\n",
    "    \"\"\"Search for a target folder in the directory structure\"\"\"\n",
    "    print(f\"Searching for '{target_folder}' from base path: {base_path}\")\n",
    "    results = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        if target_folder in dirs:\n",
    "            found_path = os.path.join(root, target_folder)\n",
    "            results.append(found_path)\n",
    "            print(f\"Found: {found_path}\")\n",
    "    \n",
    "    return results\n",
    "############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61fee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantDataset(Dataset):\n",
    "    def __init__(self, image_path, mask_path, limit=None):\n",
    "        self.image_path = image_path\n",
    "        self.mask_path = mask_path\n",
    "        self.limit = limit\n",
    "        \n",
    "        # Get image and mask files directly from the provided paths\n",
    "        self.images = sorted([os.path.join(image_path, i) for i in os.listdir(image_path)])\n",
    "        self.masks = sorted([os.path.join(mask_path, i) for i in os.listdir(mask_path)])\n",
    "        \n",
    "        if self.limit is not None:\n",
    "            self.images = self.images[:self.limit]\n",
    "            self.masks = self.masks[:self.limit]\n",
    "            \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            img = Image.open(self.images[index]).convert(\"RGB\")\n",
    "            mask = Image.open(self.masks[index]).convert(\"L\")\n",
    "            return self.transform(img), self.transform(mask)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {self.image_paths[idx]}: {e}\")\n",
    "        # Return a placeholder or the next valid image\n",
    "            return self.__getitem__((index + 1) % len(self))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86941af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        # Encoder - only 2 layers\n",
    "        self.enc1 = self._block(3, 64)\n",
    "        self.enc2 = self._block(64, 128)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = self._block(128, 256)\n",
    "        \n",
    "        # Decoder - only 2 layers\n",
    "        self.up1 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec1 = self._block(256, 128)\n",
    "        self.up2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec2 = self._block(128, 64)\n",
    "        \n",
    "        # Output\n",
    "        self.out = nn.Conv2d(64, 1, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "    def _block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder - only 2 layers\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(self.pool(enc1))\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(self.pool(enc2))\n",
    "        \n",
    "        # Decoder - only 2 layers\n",
    "        dec1 = self.up1(bottleneck)\n",
    "        dec1 = torch.cat((dec1, enc2), dim=1)\n",
    "        dec1 = self.dec1(dec1)\n",
    "        \n",
    "        dec2 = self.up2(dec1)\n",
    "        dec2 = torch.cat((dec2, enc1), dim=1)\n",
    "        dec2 = self.dec2(dec2)\n",
    "        \n",
    "        # Output\n",
    "        out = self.out(dec2)\n",
    "        return self.sigmoid(out)\n",
    "#########################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0b36834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, device, num_epochs=10):\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    loss_history=[]\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for images, masks in train_loader:\n",
    "            # Move tensors to the configured device\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        # Calculate average loss for the epoch\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        # Store the loss\n",
    "        loss_history.append(avg_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.4f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f34f09be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, dataset, device, num_samples=3):\n",
    "    model.eval()\n",
    "    \n",
    "    indices = random.sample(range(len(dataset)), num_samples)\n",
    "    # Determine number of columns based on whether overlay is shown\n",
    "    num_cols = 4 if show_overlay else 3\n",
    "    \n",
    "    plt.figure(figsize=(15, num_samples * 4))\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        image, true_mask = dataset[idx]\n",
    "        image_input = image.unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred_mask = model(image_input)\n",
    "            \n",
    "        \n",
    "        pred_mask = pred_mask.squeeze().cpu().numpy()\n",
    "        image_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "        true_mask_np = true_mask.squeeze().cpu().numpy()\n",
    "        \n",
    "        # Plot original image\n",
    "        plt.subplot(num_samples, 3, i * 3 + 1)\n",
    "        plt.imshow(image_np)\n",
    "        plt.title(\"Image\")\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        # Plot ground truth mask\n",
    "        plt.subplot(num_samples, 3, i * 3 + 2)\n",
    "        plt.imshow(true_mask_np, cmap='gray')\n",
    "        plt.title(\"Ground Truth\")\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        # Plot predicted mask\n",
    "        plt.subplot(num_samples, 3, i * 3 + 3)\n",
    "        plt.imshow(pred_mask, cmap='gray')\n",
    "        plt.title(\"Predicted Mask\")\n",
    "        plt.axis(\"off\")\n",
    "     # Plot overlay if requested\n",
    "        if show_overlay:\n",
    "            plt.subplot(num_samples, num_cols, i * num_cols + 4)\n",
    "            \n",
    "            # Create RGB overlay\n",
    "            overlay = np.zeros((image_np.shape[0], image_np.shape[1], 3))\n",
    "            \n",
    "            # Green channel for true mask\n",
    "            overlay[:, :, 1] = true_mask_np * 0.7\n",
    "            \n",
    "            # Red channel for predicted mask\n",
    "            overlay[:, :, 0] = pred_mask * 0.7\n",
    "            \n",
    "            # Yellow indicates overlap between true and predicted\n",
    "            plt.imshow(overlay)\n",
    "            plt.title(\"Overlay (Green=True, Red=Pred)\")\n",
    "            plt.axis(\"off\")\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    if loss_history is not None and len(loss_history) > 0:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, len(loss_history) + 1), loss_history, marker='o')\n",
    "        plt.title('Training Loss vs. Epochs')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "#################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21563d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(model, dataset, save_path, device):\n",
    "    model.eval()\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    for idx in range(len(dataset)):\n",
    "        image, _ = dataset[idx]\n",
    "        image_input = image.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_mask = model(image_input).squeeze().cpu().numpy()\n",
    "\n",
    "        pred_mask = (pred_mask > 0.5).astype(np.uint8) * 255\n",
    "        save_name = os.path.join(save_path, f\"mask_{idx}.png\")\n",
    "        cv2.imwrite(save_name, pred_mask)\n",
    "###################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b16698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Current working directory: /home/ubuntu/unet_env\n",
      "Searching for 'Data1' from base path: /home/ubuntu/unet_env\n",
      "Found: /home/ubuntu/unet_env/Data/Data1\n",
      "Using image path: /home/ubuntu/unet_env/Data/Data1/Natural\n",
      "Using mask path: /home/ubuntu/unet_env/Data/Data1/Mask\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 72\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete! Model saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 72\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 63\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Create and train the model\u001b[39;00m\n\u001b[1;32m     62\u001b[0m model \u001b[38;5;241m=\u001b[39m UNet()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 63\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[1;32m     66\u001b[0m save_predictions(trained_model, dataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted_masks\u001b[39m\u001b[38;5;124m\"\u001b[39m, device)\n",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, device, num_epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m masks \u001b[38;5;241m=\u001b[39m masks\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, masks)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n",
      "File \u001b[0;32m~/unet_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/unet_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[7], line 47\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m dec2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup2(dec1)\n\u001b[1;32m     46\u001b[0m dec2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((dec2, enc1), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m dec2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdec2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdec2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Output\u001b[39;00m\n\u001b[1;32m     50\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout(dec2)\n",
      "File \u001b[0;32m~/unet_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/unet_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/unet_env/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/unet_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/unet_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/unet_env/lib/python3.10/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/unet_env/lib/python3.10/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Get the current working directory\n",
    "    base_path = os.getcwd()\n",
    "    print(f\"Current working directory: {base_path}\")\n",
    "    \n",
    "    # Search for the plant_data folders\n",
    "    data_paths = find_folders(base_path, \"Data1\")\n",
    "    \n",
    "    if not data_paths:\n",
    "        # Try looking for specific folders\n",
    "        natural_paths = find_folders(base_path, \"Natural\")\n",
    "        mask_paths = find_folders(base_path, \"Mask\")\n",
    "        \n",
    "        if natural_paths and mask_paths:\n",
    "            natural_path = natural_paths[0]\n",
    "            mask_path = mask_paths[0]\n",
    "        else:\n",
    "            print(\"Could not find data folders automatically.\")\n",
    "            # Hard code the paths based on your screenshot\n",
    "            natural_path = os.path.join(base_path, \"UNET_ENV\", \"Data\", \"Data1\", \"Natural\")\n",
    "            mask_path = os.path.join(base_path, \"UNET_ENV\", \"Data\", \"Data1\", \"Mask\")\n",
    "    else:\n",
    "        # Construct paths based on found plant_data folder\n",
    "        plant_data_path = data_paths[0]\n",
    "        natural_path = os.path.join(plant_data_path, \"Natural\")\n",
    "        mask_path = os.path.join(plant_data_path, \"Mask\")\n",
    "    \n",
    "    print(f\"Using image path: {natural_path}\")\n",
    "    print(f\"Using mask path: {mask_path}\")\n",
    "    \n",
    "    # Check if directories exist\n",
    "    if not os.path.exists(natural_path): \n",
    "        print(f\"Error: Image directory not found at {natural_path}\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(mask_path):\n",
    "        print(f\"Error: Mask directory not found at {mask_path}\")\n",
    "        return\n",
    "        \n",
    "    # Create dataset and dataloader\n",
    "    dataset = PlantDataset(\n",
    "        image_path=natural_path,\n",
    "        mask_path=mask_path,\n",
    "        limit=10  # Set to an integer if you want to limit the dataset size\n",
    "    )\n",
    "    \n",
    "    # Create data loader\n",
    "    generator = torch.Generator().manual_seed(25)\n",
    "    train_loader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=4, \n",
    "        shuffle=True, \n",
    "        generator=generator,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    # Create and train the model\n",
    "    model = UNet().to(device)\n",
    "    trained_model = train_model(model, train_loader, device, num_epochs=10)\n",
    "    \n",
    "    # Save the model\n",
    "    save_predictions(trained_model, dataset, \"predicted_masks\", device)\n",
    "    visualize_predictions(trained_model, dataset, device, num_samples=3)\n",
    "\n",
    "    print(\"Training complete! Model saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unet_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
